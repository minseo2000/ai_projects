# 트랜스포머 소개

- 트랜스포머는 2017년 구글이 개발한 시퀀스 모델링을 위한 새로운 신경망 아키텍처이다.
- 이 아키텍터는 기계 번역 작업의 품질과 훈련 비용면에서 순환 신경망(RNN)을 능가했다.
- 동시에 효율적인 전이 학습 방법인 ULMFiT가 매우 크고 다양한 말뭉치에서 LSTM 신경망을 훈련해 매우 적은 양의 레이블링된 데이터로도 최고 수준의 텍스트 분류 모델을 만들어냄을 입증했다.

- 이런 발전으로 오늘날 가장 유명한 두 트랜스포머의 촉매가 되었다 바로 GPT와 BERT이다. 이 모델은 트랜스포머 아키텍처와 비지도 학습을 결합해 작업에 특화된 모델을 밑바닥부터 훈련할 필요를 없애고 거의 모든 NLP 벤치마크에서 큰 차이로 기록을 경신했다.


# 인코더 - 디코더 프레임워크
트랜스포머가 등장하기 전, NLP에서는 LSTM 같은 순환 신경망 구조가 최고 수준의 성능을 달성했다.

RNN은 단어 시퀀스를 한 언어에서 다른 언어로 매핍하는 기계 번역 시스템을 개발할 때 중요한 역할을 했다.
- 이런 종류의 작업은 개개 인코더-디코더 또는 시퀀스-투-시퀀스 구조로 처리하며 입력과 출력이 임의의 길이를 가진 시퀀스 일 때 잘 맞다.

